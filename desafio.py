# -*- coding: utf-8 -*-
"""desafio.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c97bGE6D4hGN-UyoX6h6zWVZ-g64mURc
"""

## Gostaria de deixar alguns adendos , sobre a tratativa do desafio técnico:
## Toda estrutura foi criada na Nuvem pela ferramenta Google Colab , a extensão de arquivos é .IPYNB
## Onde alguns imports são da própria ferramenta. Não iram executar diretamente no Python.
## O google solicita o authorization code , para montar a partição do Google Drive , para acesso aos arquivos
## O comando spark-subit não irá se aplicar neste caso, para execução do código.
## Qualquer dúvida , fico a disposição.
## Att. Bruno Milhati Cavallini

# Download do Pacote .Tgz do Spark
!wget -q http://www-eu.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz

#Instalação Java-8 , instalação Spark , pacotes findsaprk , collections
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!tar xf spark-2.4.4-bin-hadoop2.7.tgz
!pip install -q findspark
!pip install collections-extended

## Setando variaveis de ambiente 
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-2.4.4-bin-hadoop2.7"

# imports
import findspark
findspark.init()
import pandas as pd
import collections
from pyspark.sql import SparkSession
from pyspark import SparkContext
from pyspark import SparkConf
from pyspark.sql.functions import regexp_extract
from pyspark.sql.functions import col
from pyspark.sql.functions import sum as spark_sum
from pyspark.sql.functions import unix_timestamp, from_unixtime

# SparkContext
spark = SparkSession.builder.master("local[*]").getOrCreate()
sc = SparkContext.getOrCreate()

# Leitura de arquivos do Google Drive
# Ao executar o codigo o google solicita o authorization code , para montar a partição do Google Drive 
# Exemplo: 4/tQFvkBi4HJjeLRJMCwMOAkXIGBeMeqmpA15ir6t2i-trbmeMLYZ-lxc 

from google.colab import drive 
from google.colab import files
drive.mount('/content/gdrive')

file_loc1 = 'gdrive/My Drive/NASA_access_log_Jul95.gz'
file_loc2 = 'gdrive/My Drive/NASA_access_log_Aug95.gz'

# Rdd Texto
linesj = sc.textFile(file_loc1)
linesa = sc.textFile(file_loc2)

## Número de hosts únicos.
Julho_count = linesj.flatMap(lambda line: line.split(' ')[0]).distinct().count()
agosto_count = linesa.flatMap(lambda line: line.split(' ')[0]).distinct().count()
print('Numero de Hosts unicos em julho: %s' % Julho_count)
print('Numero de Hosts unicos em agosto: %s' % agosto_count)

##O total de erros 404.
j404j = linesj.filter(lambda x: 'HTTP/1.0" 404' in x).count()
j404a = linesa.filter(lambda x: 'HTTP/1.0" 404' in x).count()
print('O total de erros 404 ocorridos em julho: %s' % j404j)
print('O total de erros 404 ocorridos em agosto: %s' % j404a)

# DataFrame
df1 = spark.read.text(file_loc1)
df2 = spark.read.text(file_loc2)

## Expressão Regular
re_host = r'(^\S+\.[\S+\.]+\S+)\s' #Host
re_timestamp = r'\[(\d{2}/\w{3}/\d{4}:\d{2}:\d{2}:\d{2} -\d{4})]' # Timestamp
re_url = r'\"(\S+)\s(\S+)\s*(\S*)\"' #url -Requisição 
re_codigo = r'\s(\d{3})\s' # codigo http
re_bytes = r'\s(\d+)$' # bytes

sql = df1.select(regexp_extract('value', re_host, 1).alias('host'),
                         regexp_extract('value', re_timestamp, 1).alias('timestamp'),
                         regexp_extract('value', re_url, 2).alias('url'),
                         regexp_extract('value', re_codigo, 1).cast('integer').alias('codigo'),
                         regexp_extract('value', re_bytes, 1).cast('integer').alias('bytes'))

sql2 = df2.select(regexp_extract('value', re_host, 1).alias('host'),
                         regexp_extract('value', re_timestamp, 1).alias('timestamp'),
                         regexp_extract('value', re_url, 2).alias('url'),
                         regexp_extract('value', re_codigo, 1).cast('integer').alias('codigo'),
                         regexp_extract('value', re_bytes, 1).cast('integer').alias('bytes'))

df1.show(20, truncate=False)

sql.show()

##1. Número de hosts únicos. Julho e Agosto
##2. O total de erros 404. Julho e Agosto
##3. Os 5 URLs que mais causaram erro 404. Julho e Agosto
##4. Quantidade de erros 404 por dia. Julho e Agosto
##5. O total de bytes retornados.Julho e Agosto

sql.groupBy('host').count().filter('count = 1').select('host').show()
sql2.groupBy('host').count().filter('count = 1').select('host').show()

sql.groupBy('codigo').count().filter('codigo = "404"').show()
sql2.groupBy('codigo').count().filter('codigo = "404"').show()

sql.filter('codigo = "404"').groupBy('url').count().sort(col("count").desc()).show(5, truncate=False)
sql2.filter('codigo = "404"').groupBy('url').count().sort(col("count").desc()).show(5, truncate=False)

sql.filter('codigo = "404"').groupBy(sql.timestamp.substr(1,11).alias('day')).count().show()
sql2.filter('codigo = "404"').groupBy(sql2.timestamp.substr(1,11).alias('day')).count().show()

sql.select('bytes').groupBy().sum().show()
sql2.select('bytes').groupBy().sum().show()

! git clone https://github.com/souvik3333/Testing-and-Debugging-Tools